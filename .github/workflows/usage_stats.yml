name: earth_engine_stats
on:
  workflow_dispatch:
  schedule:
    - cron: "0 14 * * *"  # Run daily at 14:00 UTC (9:00 AM CDT)

jobs:
  build:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write  # Allow the action to commit and push changes
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          
      - name: Setup Google Cloud CLI
        uses: google-github-actions/setup-gcloud@v2
        with:
          version: 'latest'
          
      - name: Create service account key file
        run: |
          echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' > /tmp/gcp-key.json
          
      - name: Authenticate to Google Cloud
        run: |
          gcloud auth activate-service-account --key-file=/tmp/gcp-key.json
          gcloud config set project $(cat /tmp/gcp-key.json | jq -r '.project_id')
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas
          sudo apt-get update
          sudo apt-get install -y jq
          
      - name: Generate Earth Engine Stats JSON
        uses: jannekem/run-python-script-action@v1
        with:
          script: |
            import json
            import logging
            import os
            import subprocess
            import tempfile
            from collections import defaultdict
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from datetime import date, datetime, timedelta
            from typing import Dict, List, Optional
            import pandas as pd

            # Set up logging
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
            logger = logging.getLogger(__name__)

            class EarthEngineStatsProcessor:
                """Process Earth Engine statistics and generate JSON data."""

                def __init__(self):
                    """Initialize the processor."""
                    self.bucket_name = "earthengine-stats"
                    self.base_path = "providers/sat-io"
                    self.start_date = date(2024, 4, 23)
                    self.data = pd.DataFrame()
                    self.processed_stats = {}

                def _generate_date_range(self, end_date: Optional[date] = None) -> List[date]:
                    """Generate list of dates."""
                    if end_date is None:
                        end_date = date.today() - timedelta(days=1)
                    return [self.start_date + timedelta(days=x) for x in range((end_date - self.start_date).days + 1)]

                def _download_csv_data(self, target_date: date) -> Optional[pd.DataFrame]:
                    """Download and parse CSV data for a specific date."""
                    filename = f"earthengine_stats_{target_date.strftime('%Y-%m-%d')}.csv"
                    gs_path = f"gs://{self.bucket_name}/{self.base_path}/{filename}"

                    try:
                        with tempfile.NamedTemporaryFile(mode='w+', suffix='.csv', delete=False) as temp_file:
                            temp_path = temp_file.name

                        result = subprocess.run(
                            ['gcloud', 'storage', 'cp', gs_path, temp_path],
                            capture_output=True, text=True, timeout=30, check=False
                        )

                        if result.returncode != 0:
                            if "No such object" in result.stderr or "Not Found" in result.stderr:
                                logger.debug(f"File not found: {gs_path}")
                            else:
                                logger.error(f"gcloud error for {filename}: {result.stderr.strip()}")
                            os.unlink(temp_path)
                            return None

                        df = pd.read_csv(temp_path)
                        if df.empty:
                            os.unlink(temp_path)
                            return None

                        df['file_date'] = pd.to_datetime(target_date)
                        df['30-day active users'] = pd.to_numeric(df['30-day active users'], errors='coerce')
                        logger.debug(f"Successfully downloaded {len(df)} records from {filename}")
                        os.unlink(temp_path)
                        return df

                    except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
                        logger.error(f"Error downloading {filename}: {e}")
                        if 'temp_path' in locals() and os.path.exists(temp_path):
                            os.unlink(temp_path)
                        return None

                def collect_all_data(self, end_date: Optional[date] = None, max_workers: int = 12) -> None:
                    """Collect data from all available dates using parallel processing."""
                    dates = self._generate_date_range(end_date)
                    logger.info(f"Collecting data for {len(dates)} dates using {max_workers} workers.")

                    all_dataframes = []
                    successful_downloads = 0
                    
                    with ThreadPoolExecutor(max_workers=max_workers) as executor:
                        future_to_date = {executor.submit(self._download_csv_data, target_date): target_date for target_date in dates}
                        for i, future in enumerate(as_completed(future_to_date)):
                            df = future.result()
                            if df is not None:
                                all_dataframes.append(df)
                                successful_downloads += 1
                                
                            if (i + 1) % 50 == 0 or (i + 1) == len(dates):
                                logger.info(f"Download progress: {i + 1}/{len(dates)} days processed, {successful_downloads} successful downloads.")

                    if all_dataframes:
                        self.data = pd.concat(all_dataframes, ignore_index=True).sort_values('file_date')
                        logger.info(f"Total records collected: {len(self.data)} from {successful_downloads} days")
                    else:
                        logger.warning("No data was collected.")
                        self.data = pd.DataFrame()

                def _generate_daily_data_format(self) -> Dict:
                    """Generate daily data in the format expected by the HTML template."""
                    if self.data.empty:
                        return {}

                    df = self.data.copy()
                    df['Dataset'] = df['Dataset'].str.replace(r'/\[\d+\s+assets\]$', '', regex=True)
                    df['users'] = df['30-day active users'].fillna(0).astype(int)
                    
                    daily_data = {}
                    
                    # Group by date and dataset to create the nested structure
                    for date_str, date_group in df.groupby(df['file_date'].dt.strftime('%Y-%m-%d')):
                        daily_data[date_str] = {}
                        for _, row in date_group.iterrows():
                            daily_data[date_str][row['Dataset']] = row['users']
                    
                    return daily_data

                def _calculate_statistics(self) -> Dict:
                    """Calculate comprehensive statistics for the report."""
                    if self.data.empty:
                        return {}

                    df = self.data.copy()
                    df['Dataset'] = df['Dataset'].str.replace(r'/\[\d+\s+assets\]$', '', regex=True)
                    df['users'] = df['30-day active users'].fillna(0).astype(int)

                    # Generate daily data format for template
                    daily_data = self._generate_daily_data_format()

                    # Daily Trends for internal calculations
                    daily_trends = df.groupby('file_date').agg(
                        total_users=('users', 'sum'),
                        dataset_count=('Dataset', 'nunique')
                    ).reset_index()
                    daily_trends['avg_users'] = (daily_trends['total_users'] / daily_trends['dataset_count']).round(2)

                    # Dataset Rankings
                    dataset_rankings = df.groupby('Dataset').agg(
                        total_users=('users', 'sum'),
                        avg_users=('users', 'mean'),
                        appearances=('file_date', 'count'),
                        std_users=('users', 'std')
                    ).sort_values('total_users', ascending=False).fillna(0)

                    # Summary Stats
                    total_users = df['users'].sum()
                    total_datasets = df['Dataset'].nunique()
                    total_usage_events = len(df)
                    avg_users_per_dataset = dataset_rankings['avg_users'].mean()

                    # Growth Rate
                    first_day_users = daily_trends.iloc[0]['total_users']
                    last_day_users = daily_trends.iloc[-1]['total_users']
                    growth_rate = ((last_day_users - first_day_users) / first_day_users * 100) if first_day_users > 0 else 0

                    # Peak Day
                    peak_day_row = daily_trends.loc[daily_trends['total_users'].idxmax()]
                    peak_day = {
                        'date': peak_day_row['file_date'].strftime('%Y-%m-%d'),
                        'total_users': int(peak_day_row['total_users']),
                        'dataset_count': int(peak_day_row['dataset_count'])
                    }

                    # Peak Dataset
                    peak_dataset_name = dataset_rankings.index[0]
                    peak_dataset = {
                        'name': peak_dataset_name,
                        'total_users': int(dataset_rankings.iloc[0]['total_users']),
                        'avg_users': round(dataset_rankings.iloc[0]['avg_users'], 2)
                    }

                    # Weekly Trends
                    daily_trends['week_label'] = daily_trends['file_date'].dt.strftime('%Y-W%U')
                    weekly_trends = daily_trends.groupby('week_label').agg(
                        total_users=('total_users', 'sum'),
                        avg_users=('total_users', 'mean'),
                        unique_datasets=('dataset_count', 'max'),
                        start_date=('file_date', 'min'),
                        end_date=('file_date', 'max')
                    )

                    # Monthly Trends
                    daily_trends['month_label'] = daily_trends['file_date'].dt.strftime('%Y-%m')
                    monthly_trends = daily_trends.groupby('month_label').agg(
                        total_users=('total_users', 'sum'),
                        avg_users=('total_users', 'mean'),
                        std_users=('total_users', 'std'),
                        unique_datasets=('dataset_count', 'max')
                    ).fillna(0)

                    stats = {
                        "daily_data": daily_data,  # This is the key format expected by the template
                        "summary": {
                            "total_datasets": int(total_datasets),
                            "total_usage_events": int(total_usage_events),
                            "total_users": int(total_users),
                            "avg_users_per_dataset": round(avg_users_per_dataset, 2),
                            "date_range": {
                                "start": df['file_date'].min().strftime('%Y-%m-%d'),
                                "end": df['file_date'].max().strftime('%Y-%m-%d')
                            },
                            "growth_rate": round(growth_rate, 2)
                        },
                        "peaks": {
                            "peak_day": peak_day, 
                            "peak_dataset": peak_dataset
                        },
                        "dataset_rankings": {
                            index: {
                                'total_users': int(row.total_users),
                                'avg_users': round(row.avg_users, 2),
                                'appearances': int(row.appearances),
                                'std_users': round(row.std_users, 2)
                            } for index, row in dataset_rankings.iterrows()
                        },
                        "daily_trends": {
                            row.file_date.strftime('%Y-%m-%d'): {
                                'total_users': int(row.total_users), 
                                'avg_users': round(row.avg_users, 2), 
                                'dataset_count': int(row.dataset_count)
                            } for row in daily_trends.itertuples()
                        },
                        "weekly_trends": {
                            index: {
                                'total_users': int(row.total_users),
                                'avg_users': round(row.avg_users, 2),
                                'unique_datasets': int(row.unique_datasets),
                                'date_range': f"{row.start_date.strftime('%Y-%m-%d')} to {row.end_date.strftime('%Y-%m-%d')}"
                            } for index, row in weekly_trends.iterrows()
                        },
                        "monthly_trends": {
                            index: {
                                'total_users': int(row.total_users),
                                'avg_users': round(row.avg_users, 2),
                                'std_users': round(row.std_users, 2),
                                'unique_datasets': int(row.unique_datasets)
                            } for index, row in monthly_trends.iterrows()
                        }
                    }
                    
                    self.processed_stats = stats
                    return stats

                def generate_json_output(self, output_file: str = "object_stats.json") -> None:
                    """Generate JSON output file."""
                    if not self.processed_stats:
                        logger.error("No processed stats available. Run _calculate_statistics first.")
                        return

                    try:
                        with open(output_file, 'w', encoding='utf-8') as f:
                            json.dump(self.processed_stats, f, indent=2, ensure_ascii=False)
                        logger.info(f"Successfully generated JSON output: {output_file}")
                    except IOError as e:
                        logger.error(f"Failed to write JSON output to {output_file}: {e}")

                def process_and_generate_json(self, end_date: Optional[date] = None,
                                              output_file: str = "object_stats.json",
                                              max_workers: int = 12) -> None:
                    """Main method to process data and generate JSON output."""
                    logger.info("Starting Earth Engine stats processing...")
                    self.collect_all_data(end_date, max_workers)

                    if self.data.empty:
                        logger.error("No data collected. Aborting JSON generation.")
                        return

                    logger.info("Calculating statistics...")
                    self._calculate_statistics()

                    logger.info("Generating JSON output...")
                    self.generate_json_output(output_file)

                    # Log some summary information
                    logger.info(f"Processing complete. Summary:")
                    logger.info(f"  - Total datasets: {self.processed_stats['summary']['total_datasets']}")
                    logger.info(f"  - Total users: {self.processed_stats['summary']['total_users']:,}")
                    logger.info(f"  - Date range: {self.processed_stats['summary']['date_range']['start']} to {self.processed_stats['summary']['date_range']['end']}")
                    logger.info(f"  - Growth rate: {self.processed_stats['summary']['growth_rate']:.2f}%")

            # Main execution
            processor = EarthEngineStatsProcessor()
            processor.process_and_generate_json(end_date=None, output_file="object_stats.json", max_workers=12)
            
      - name: Verify JSON output
        run: |
          if [ -f "object_stats.json" ]; then
            echo "JSON file generated successfully"
            echo "File size: $(stat -c%s object_stats.json) bytes"
            echo "First few lines:"
            head -10 object_stats.json
          else
            echo "ERROR: JSON file was not generated"
            exit 1
          fi
          
      - name: Commit and push changes
        run: |
          export TZ="America/Chicago"
          today=$(date +"%Y-%m-%d %H:%M CDT")
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add object_stats.json
          git commit -m "Updated Earth Engine stats ${today}" -a
          git push
          
      - name: Cleanup
        if: always()
        run: |
          rm -f /tmp/gcp-key.json
